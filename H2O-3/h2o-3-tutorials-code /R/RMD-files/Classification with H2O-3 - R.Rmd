---
title: "Classification Tutorial with H2O-3 Using R"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. To execute a code chunk, click *Run* (play) button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 


## Task 1: Initial Setup

```{r}
library(h2o)
library(tidyverse)
library(DT)

h2o.init(bind_to_localhost = FALSE, context_path = "h2o")
```

```{r}
h2o.no_progress()
```

Load the data
```{r}
loan_level <- h2o.importFile(path = "https://s3.amazonaws.com/data.h2o.ai/DAI-Tutorials/loan_level_500k.csv")
```

## Task 2: Machine Learning Concepts - See Tutorial

## Task 3: Start Experiment

```{r}
h2o.head(loan_level) %>% as_tibble()
h2o.describe(loan_level) %>% as_tibble()
```

```{r}
h2o.table(loan_level[, c("DELINQUENT")])
```

```{r}
splits <- h2o.splitFrame(loan_level, c(0.7, 0.15), seed = 42)

train <- splits[[1]]
valid <- splits[[2]]
test <- splits[[3]]

h2o.describe(train) %>% as_tibble()
```

```{r}
nrow(train)
nrow(valid)
nrow(test)
```

```{r}
ignore <- c("DELINQUENT", "PREPAID", "PREPAYMENT_PENALTY_MORTGAGE_FLAG", "PRODUCT_TYPE")

y <- "DELINQUENT"

x <- setdiff(colnames(train), ignore)
x
```

## Task 4: build a GLM

```{r}
glm <- h2o.glm(x = x, y = y, training_frame = train, validation_frame = valid, 
    seed = 42, model_id = "default_glm", family = c("binomial"), score_each_iteration = TRUE)

glm
```

```{r}
h2o.varimp_plot(glm) #plot variable importance
```

```{r}
train_glm_perf <- h2o.performance(glm, train) #save model performance on training set
valid_glm_perf <- h2o.performance(glm, valid) #save model performance on validation set
```

We can print all the metrics from either of our performance
```{r}
train_glm_perf
```

We can print individual metrics
```{r}
train_glm_perf@metrics$max_criteria_and_metric_scores$threshold[4]
```

print the training accuracy at an specific threshold (max threshold)
```{r}
h2o.accuracy(train_glm_perf, thresholds = train_glm_perf@metrics$max_criteria_and_metric_scores$threshold[4])
```

print the validation accuracy at an specific threshold
```{r}
h2o.accuracy(valid_glm_perf, thresholds = valid_glm_perf@metrics$max_criteria_and_metric_scores$threshold[4])
```

Printing and plotting validation AUC
```{r}
h2o.auc(valid_glm_perf)
plot(valid_glm_perf)
```

```{r}
h2o.predict(glm, valid)
```

## Task 5: Build a Random Forest

```{r}
rf <- h2o.randomForest(x = x, y = y, training_frame = train, validation_frame = valid, 
    seed = 42, model_id = "default_rf")

rf
```

```{r}
plot(rf, metric='AUC')
```
```{r}
h2o.varimp_plot(rf)
```

Exploring training metrics
```{r}
train_rf_perf <- h2o.performance(rf, train)
h2o.accuracy(train_rf_perf, thresholds = train_rf_perf@metrics$max_criteria_and_metric_scores$threshold[4])
h2o.auc(train_rf_perf)
```

Validation metrics
```{r}
valid_rf_perf <- h2o.performance(rf, valid)
h2o.accuracy(valid_rf_perf, threshold = valid_rf_perf@metrics$max_criteria_and_metric_scores$threshold[4])
h2o.auc(valid_rf_perf)
plot(valid_rf_perf)
```

```{r}
h2o.predict(rf, valid)
```

## Task 6: Build a GBM

```{r}
gbm <- h2o.gbm(x = x, y = y, training_frame = train, validation_frame = valid, 
    seed = 42, model_id = "default_gbm")

summary(gbm)
```

```{r}
h2o.predict(gbm, valid)
```

```{r}
valid_gbm_perf <- h2o.performance(gbm, valid)
```

```{r}
h2o.auc(valid_gbm_perf)
h2o.F1(valid_gbm_perf, thresholds = valid_gbm_perf@metrics$max_criteria_and_metric_scores$threshold[1])
```

## Task 7: Tune the GLM with H2O Grid Search

Save a sequence for alpha to use in grid search
```{r}
glm_alpha <- seq(from = 0, to = 1, by = 0.01)

#Set up grid search
glm_grid <- h2o.grid(algorithm = "glm", grid_id = "random_glm_grid", family = "binomial", 
                     lambda_search = TRUE, seed = 42,
                     
                     hyper_params = list(
                       alpha = glm_alpha,
                       missing_values_handling = c("MeanImputation", "Skip")
                       ),
                     
                     x = x, 
                     y = y, 
                     training_frame = train, 
                     validation_frame = valid,
                     
                     lambda_search = TRUE,
                     
                     search_criteria = list(
                       strategy = "RandomDiscrete",
                       max_runtime_secs = 300,
                       seed = 42
                     )
)
```


Retrieve all the models from the grid search using AUC as sort metric
```{r}
gml_grid_auc <- h2o.getGrid(grid_id = "random_glm_grid", sort_by = "auc", decreasing = TRUE)
as.data.frame(gml_grid_auc@summary_table)
```

Retirieve the best model
```{r}
tuned_glm <- h2o.getModel(gml_grid_auc@model_ids[[1]]) #getting the best model
tuned_glm
```

Save model performance on validation set
```{r}
valid_tuned_glm_perf <- h2o.performance(tuned_glm, valid)
```

Print the AUC for default and tuned model
```{r}
h2o.auc(valid_glm_perf) #default model perf
h2o.auc(valid_tuned_glm_perf) #tuned model perf
```

Print both F1 scores
```{r}
h2o.F1(valid_glm_perf, thresholds = valid_glm_perf@metrics$max_criteria_and_metric_scores$threshold[1])
h2o.F1(valid_tuned_glm_perf, thresholds = valid_tuned_glm_perf@metrics$max_criteria_and_metric_scores$threshold[1])
```

Print confusion matrix
```{r}
h2o.confusionMatrix(valid_glm_perf)
h2o.confusionMatrix(valid_tuned_glm_perf)
```

## Task 8: Tune the RF model with H2O Grid Search

```{r}
rf_depth_grid <- h2o.grid(algorithm = "randomForest", grid_id = "rf_depth_grid",
                          seed = 42, stopping_rounds = 5, stopping_metric = "AUC", stopping_tolerance = 1e-4, 
            
                          hyper_params = list(
                            max_depth = c(1, 3, 5, 6, 7, 8, 9, 10, 12, 13, 15, 20, 25, 35)
                          ),
                          
                          search_criteria = list(
                            strategy = "Cartesian"
                          ),
                          
                          x = x, 
                          y = y, 
                          training_frame = train, 
                          validation_frame = valid

)
```

```{r}
rf_depth_grid_auc <- h2o.getGrid(grid_id = "rf_depth_grid", sort_by = "auc", decreasing = TRUE)
as.data.frame(rf_depth_grid_auc@summary_table)
```

```{r}
rf_sample_rate <- seq(from = 0.2, to = 1, by = 0.01)

rf_random_grid <- h2o.grid(algorithm = "randomForest", grid_id = "rf_random_search", ntrees = 500,
                          seed = 42, stopping_rounds = 5, stopping_metric = "AUC", stopping_tolerance = 1e-3, 
                          
                          hyper_params = list(
                            max_depth = c(8, 9, 10, 11, 12),
                            sample_rate = rf_sample_rate
                          ),
                          
                          search_criteria = list(
                            strategy = "RandomDiscrete",
                            max_runtime_secs = 900,
                            seed = 42
                          ),
                          
                          x = x, 
                          y = y, 
                          training_frame = train, 
                          validation_frame = valid
                          
)
```

```{r}
rf_random_grid_auc <- h2o.getGrid(grid_id = "rf_random_search", sort_by = "auc", decreasing = TRUE)
as.data.frame(rf_random_grid_auc@summary_table)
```

```{r}
tuned_rf <- h2o.getModel(rf_random_grid_auc@model_ids[[1]]) #getting the best model
tuned_rf
```

```{r}
valid_tuned_rf_perf <- h2o.performance(tuned_rf, valid)
```

```{r}
h2o.auc(valid_tuned_rf_perf)
h2o.F1(valid_tuned_rf_perf, thresholds = valid_tuned_rf_perf@metrics$max_criteria_and_metric_scores$threshold[1])
```

```{r}
h2o.auc(valid_rf_perf)
h2o.F1(valid_rf_perf, thresholds = valid_rf_perf@metrics$max_criteria_and_metric_scores$threshold[1])
```

```{r}
h2o.confusionMatrix(valid_tuned_rf_perf)
h2o.confusionMatrix(valid_rf_perf)
```

## Task 9: Tune the GBM with H2O GridSearch

```{r}
gbm_depth_grid <- h2o.grid(algorithm = "gbm", grid_id = "gbm_depth_grid",
                          seed = 42, stopping_rounds = 5, stopping_metric = "AUC", stopping_tolerance = 1e-4, 
                          
                          hyper_params = list(
                            max_depth = c(3,4,5,6,7,8,9,10,12,13,15)
                          ),
                          
                          search_criteria = list(
                            strategy = "Cartesian"
                          ),
                          
                          x = x, 
                          y = y, 
                          training_frame = train, 
                          validation_frame = valid
                          
)
```

```{r}
gbm_depth_grid_auc <- h2o.getGrid(grid_id = "gbm_depth_grid", sort_by = "auc", decreasing = TRUE)
as.data.frame(gbm_depth_grid_auc@summary_table)
```

```{r}
gbm_sample_rate <- seq(from = 0.2, to = 1, by = 0.01)
gbm_col_sample_rate <- seq(from = 0.2, to = 1, by = 0.01)
gbm_col_sample_rate_per_tree <- seq(from = 0.2, to = 1, by = 0.01)
gbm_col_sample_rate_change_per_level <- seq(from = 0.9, to = 1.1, by = 0.01)


gbm_random_grid_2 <- h2o.grid(algorithm = "gbm", grid_id = "gbm_random_search", ntrees = 500, learn_rate = 0.05,
                           seed = 42, stopping_rounds = 5, stopping_metric = "AUC", stopping_tolerance = 1e-3, 
                           
                           hyper_params = list(
                             max_depth = c(4, 5, 6, 7, 8),
                             sample_rate = gbm_sample_rate,
                             col_sample_rate = gbm_col_sample_rate,
                             col_sample_rate_per_tree = gbm_col_sample_rate_per_tree,
                             col_sample_rate_change_per_level = gbm_col_sample_rate_change_per_level
                             
                           ),
                           
                           search_criteria = list(
                             strategy = "RandomDiscrete",
                             max_runtime_secs = 900,
                             seed = 42
                           ),
                           
                           x = x, 
                           y = y, 
                           training_frame = train, 
                           validation_frame = valid
                           
)
```

```{r}
summary(gbm_random_grid_2, show_stack_traces = TRUE)
```

```{r}
gbm_random_grid_auc <- h2o.getGrid(grid_id = "gbm_random_search", sort_by = "auc", decreasing = TRUE)
as.data.frame(gbm_random_grid_auc@summary_table)
```

```{r}
tuned_gbm <- h2o.getModel(gbm_random_grid_auc@model_ids[[1]])
tuned_gbm
```

```{r}
valid_tuned_gbm_perf <- h2o.performance(tuned_gbm, valid)
```

validation perf on tuned model
```{r}
h2o.auc(valid_tuned_gbm_perf)
h2o.F1(valid_tuned_gbm_perf, thresholds = valid_tuned_gbm_perf@metrics$max_criteria_and_metric_scores$threshold[1])
```


validation perf on tuned model
```{r}
h2o.auc(valid_gbm_perf)
h2o.F1(valid_gbm_perf, thresholds = valid_gbm_perf@metrics$max_criteria_and_metric_scores$threshold[1])
```

Comparing default and tuned perfomance
```{r}
h2o.confusionMatrix(valid_tuned_gbm_perf)
h2o.confusionMatrix(valid_gbm_perf)
```

## Task 10: Test Performance

Save test performance of each model
```{r}
tuned_glm_test_perf <- h2o.performance(tuned_glm, test)
tuned_rf_test_perf  <- h2o.performance(tuned_rf, test)
tuned_gbm_test_perf <- h2o.performance(tuned_gbm, test)
```

Print the test AUC
```{r}
h2o.auc(tuned_glm_test_perf)
h2o.auc(tuned_rf_test_perf)
h2o.auc(tuned_gbm_test_perf)
```

Print the test Confusion Matrix
```{r}
h2o.confusionMatrix(tuned_glm_test_perf)
h2o.confusionMatrix(tuned_rf_test_perf)
h2o.confusionMatrix(tuned_gbm_test_perf)
```

Print the test F1 score
```{r}
h2o.F1(tuned_glm_test_perf, thresholds =  tuned_glm_test_perf@metrics$max_criteria_and_metric_scores$threshold[1])
h2o.F1(tuned_rf_test_perf, thresholds = tuned_rf_test_perf@metrics$max_criteria_and_metric_scores$threshold[1])
h2o.F1(tuned_gbm_test_perf, thresholds = tuned_gbm_test_perf@metrics$max_criteria_and_metric_scores$threshold[1])
```
## Task 11: Challenge
```{r}
h2o.shutdown()
```






